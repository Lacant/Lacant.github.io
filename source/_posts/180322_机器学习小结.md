---
title: Review:Linear Regression VS Logical Regression
date: 2018-3-22 15:55:29
tags: [机器学习,高数]
categories: 网络技术 
toc: true
mathjax: true
---

吴坦恩机器学习课程小结

<!-- more -->

## 第六课

- 线性回归与逻辑回归相关函数对比：

<style>
table th:first-of-type {
    width: 130px;
}
</style>

|         | Linear Regression    |  Logical Regression  |
| -----   | :-----------------   | :----------------- |
| Hypothesis function | $h_\theta(x) = \theta_0+ \sum_{i=1}^n \theta_ix_i$     |   /   |
| 矩阵型式            | $h_\theta(x) = \theta^Tx$      | $h_\theta(x) = \frac{1}{1+ e^-\theta^T x}$    |
| Cost function       | $J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^i) - y^i)^2$      | $J(\theta)= -\frac{1}{m} [\sum_{i=1}^m y^ilogh_\theta(x^i)+ (1-y^i)log(1-h_\theta(x^i))]$    |
| Gradient descent    | $\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^i)-y^i)x_j^i$        | $\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^i)-y^i)x_j^i$                     |
|                     | $\theta_j := \theta_j -\alpha\frac{\partial}{\partial\theta_j}J(\theta)$ | $\theta_j := \theta_j -\alpha\frac{\partial}{\partial\theta_j}J(\theta)$ |
| Regularization      | $J(\theta) = \frac{1}{2m} [\sum_{i=1}^m (h_\theta(x^i) - y^i)^2 + \lambda \sum_{i=1}^n\theta_j^2]$ |              |

- 公式解析
	- Hypothesis function是预测函数，Cost function是代价函数，代价函数$J(\theta)$是指所有样本数据与$h_\theta(x)$的偏差，所以这里有个sum的求和;
	- 梯度下降是为了求$J(\theta)$的最小值，也就是意味着求出参数$\theta$,使得偏差$J(\theta)$最小;
	- 梯度下降算法的计算过程，实际上是每次都将所有样本数据代入计算公式，进行迭代计算，直至参数$\theta$的变化率趋于0;
	<img src="https://github.com/Lacant/test01/raw/master/costfunction.jpeg" alt="machinelearning" title="costFunction" width="150" height="200" />




- 在逻辑回归中，Hypothesis function的值代表着y=1的概率: $h_\theta(x) = P(y=1|x;\theta)$

- Octave求逻辑回归，梯度下降代码：
	- 定义costFunction
			$ function [jVal, gradient] = costFunction(theta)
			$ jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
			$ gradient = zeros (2,1);
			$ gradient(1) = 2* (theta(1)-5);
			$ gradient(2) = 2*（theta(2)-5);
	
	- 调用fminunc函数进行梯度迭代计算：		
			$ options = optimset('GradObj', 'on', 'MaxIter', '100')
			$ initialTheta = zeros(2,1);
			$ [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
			$ options = optimset('GradObj', 'on', 'MaxIter', '100');
			$ initialTheta = zeros(2,1);
			$ [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
